{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a34c57c2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.01163,
     "end_time": "2025-03-09T09:46:30.208364",
     "exception": false,
     "start_time": "2025-03-09T09:46:30.196734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In this file is provided the custom CNN and all the regularization techniques are included.\n",
    "\n",
    "# Run time: 40 minute(depends on early stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e4f136",
   "metadata": {
    "papermill": {
     "duration": 14.45287,
     "end_time": "2025-03-09T09:46:44.665845",
     "exception": false,
     "start_time": "2025-03-09T09:46:30.212975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# Used libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, Input, MaxPooling2D,Dropout,BatchNormalization,Reshape\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d17e3",
   "metadata": {
    "papermill": {
     "duration": 0.009575,
     "end_time": "2025-03-09T09:46:44.680074",
     "exception": false,
     "start_time": "2025-03-09T09:46:44.670499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Impose seed for the reproducibility\n",
    "seed = 42\n",
    "keras.utils.set_random_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853a32bb",
   "metadata": {
    "papermill": {
     "duration": 0.008337,
     "end_time": "2025-03-09T09:46:44.692553",
     "exception": false,
     "start_time": "2025-03-09T09:46:44.684216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dimension of the resized images and input dimention of the CNN\n",
    "Target_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4004a0a8",
   "metadata": {
    "papermill": {
     "duration": 0.008677,
     "end_time": "2025-03-09T09:46:44.705406",
     "exception": false,
     "start_time": "2025-03-09T09:46:44.696729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths of the folder of the images and labels, after use to access the files uploaded in the Input\n",
    "path = \"/kaggle/input\"\n",
    "train_path = path + \"/Train\"\n",
    "test_path = path + \"/Test\"\n",
    "test_csv_path = path + \"/Test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccdfcae",
   "metadata": {
    "papermill": {
     "duration": 0.913947,
     "end_time": "2025-03-09T09:46:45.623478",
     "exception": false,
     "start_time": "2025-03-09T09:46:44.709531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of images path to load them\n",
    "img_list = []\n",
    "label_train_list = []\n",
    "folders = os.listdir(train_path)  # directory of the folder containing all the train images\n",
    "\n",
    "for folder in folders:  # access singularly the 43 folders contained in the train folder\n",
    "    for img in os.listdir(train_path + \"/\"+folder): # access singlularly every image contained in the folder\n",
    "        img_list.append  (train_path +  \"/\"+ folder+\"/\"+img) # creates an array with all the images in order of access\n",
    "        label_train_list.append(folder)                      # creates at the same time an array with the number of the folder accessed (corresponds to the class)\n",
    "\n",
    "# Sorted numpy array label of the corresponding image\n",
    "label_train_list = np.array(label_train_list, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d41aef",
   "metadata": {
    "papermill": {
     "duration": 0.011559,
     "end_time": "2025-03-09T09:46:45.639526",
     "exception": false,
     "start_time": "2025-03-09T09:46:45.627967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dictionary used for the print of images with the number of the class and the corresponding name\n",
    "label_name={\n",
    "0: \"Speed Limit 20\",  \n",
    "1: \"Speed Limit 30\",  \n",
    "2: \"Speed Limit 50\",  \n",
    "3: \"Speed Limit 60\",  \n",
    "4: \"Speed Limit 70\",  \n",
    "5: \"Speed Limit 80\",  \n",
    "6: \"End of a Speed Limit 80\",  \n",
    "7: \"Speed Limit 100\",  \n",
    "8: \"Speed Limit 120\",  \n",
    "9: \"No overtaking\",  \n",
    "10: \"No overtaking by trucks\",  \n",
    "11: \"Crossroads\",  \n",
    "12: \"Priority Road\",  \n",
    "13: \"Give way\",  \n",
    "14: \"Stop\", \n",
    "15: \"All vehicles prohibited in both directions\",  \n",
    "16: \"No trucks\",  \n",
    "17: \"No Entry\",  \n",
    "18: \"Other Hazards\",  \n",
    "19: \"Curve to left\",  \n",
    "20: \"Curve to right\",  \n",
    "21: \"Double curve, first to the left\",  \n",
    "22: \"Uneven Road\",  \n",
    "23: \"Slippery Road\",  \n",
    "24: \"Road Narrows Near Side\",  \n",
    "25: \"Roadworks\",  \n",
    "26: \"Traffic lights\",  \n",
    "27: \"No pedestrians\",  \n",
    "28: \"Children\",  \n",
    "29: \"Cycle Route\",  \n",
    "30: \"Be careful in winter\",  \n",
    "31: \"Wild animals\",  \n",
    "32: \"No parking\",  \n",
    "33: \"Turn right ahead\",  \n",
    "34: \"Turn left ahead\",  \n",
    "35: \"Ahead Only\",  \n",
    "36: \"Proceed straight or turn right\",  \n",
    "37: \"Proceed straight or turn left\",  \n",
    "38: \"Pass onto right\",  \n",
    "39: \"Pass onto left\",  \n",
    "40: \"Roundabout\",  \n",
    "41: \"No overtaking\",  \n",
    "42: \"End of Truck Overtaking Prohibition\",  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e66f99e",
   "metadata": {
    "papermill": {
     "duration": 0.790251,
     "end_time": "2025-03-09T09:46:46.433765",
     "exception": false,
     "start_time": "2025-03-09T09:46:45.643514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset visualization -> print of an histogram\n",
    "\n",
    "df=pd.DataFrame({\"img\":img_list,\"label\":label_train_list})     # define a type of data formed by couples\n",
    "df[\"label\"]=df[\"label\"].astype(int)                            # changes the type of the label column\n",
    "df[\"actual_label\"]=df[\"label\"].map(label_name)                 # creates a new column with the name of the classes mapped exaclty in the row of the corresponding clas\n",
    "\n",
    "plt.figure(figsize=(23,8))                                            # create the images\n",
    "\n",
    "ax = sns.countplot(      x = df[\"actual_label\"],                      # on the x axis the names of the classes\n",
    "                   palette = \"viridis\",                               # how to cloror the histogram\n",
    "                     order = df['actual_label'].value_counts().index) # counts the number of elements with the same name of class\n",
    "\n",
    "for p in ax.containers:\n",
    "    ax.bar_label(p, fontsize=12, color='black', padding=5)            # for each column of the histogram add a label of the corresponding value\n",
    "    \n",
    "plt.xticks(rotation=90);                                              # rotates the graphs of 90Â°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf19080f",
   "metadata": {
    "papermill": {
     "duration": 1.142182,
     "end_time": "2025-03-09T09:46:47.582829",
     "exception": false,
     "start_time": "2025-03-09T09:46:46.440647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function for loading and resizing the images  \n",
    "def process_image(img_path, label):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)  # decode image as RGB\n",
    "    img = tf.image.resize(img, [Target_size, Target_size], method=tf.image.ResizeMethod.BILINEAR)\n",
    "    img = img / 255.0  # Normalization\n",
    "    return img, label\n",
    "\n",
    "# Create a dataset with images and labels\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_list, label_train_list)) #It takes each pair (image, label) and transforms it into a dataset of individual elements.\n",
    "\n",
    "# Pre-processing of the dataset\n",
    "dataset = dataset.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# with .map tensorflow apply at each couple image+label in dataset the function process_image\n",
    "# num_parallel_calls specifies how many parallel threads to use to apply the process_image function.\n",
    "# Using tf.data.AUTOTUNE, TensorFlow automatically optimizes the number of threads based on available hardware resources \n",
    "# to maximize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa91de",
   "metadata": {
    "papermill": {
     "duration": 1.547945,
     "end_time": "2025-03-09T09:46:49.137513",
     "exception": false,
     "start_time": "2025-03-09T09:46:47.589568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model definition\n",
    "\n",
    "l2_lambda = 0.0001  # value for the regularizer\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Input(shape=(Target_size,Target_size,3)))\n",
    "\n",
    "model.add(Conv2D(64,kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "model.add(Conv2D(128,kernel_size=(3,3),activation='relu',padding='same', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256,kernel_size=(3,3),activation='relu',padding='same', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "model.add(Conv2D(512,kernel_size=(3,3),activation='relu',padding='same', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dense(128,activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(43,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32682809",
   "metadata": {
    "papermill": {
     "duration": 0.033115,
     "end_time": "2025-03-09T09:46:49.180645",
     "exception": false,
     "start_time": "2025-03-09T09:46:49.147530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff25f0b",
   "metadata": {
    "papermill": {
     "duration": 0.020828,
     "end_time": "2025-03-09T09:46:49.209354",
     "exception": false,
     "start_time": "2025-03-09T09:46:49.188526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model compilation\n",
    "opt      = keras.optimizers.Adam(learning_rate = 1e-3, ema_momentum=0.95)     # Adam used as pÃ¬optimizez\n",
    "loss_fcn = keras.losses.SparseCategoricalCrossentropy()                       # Sparse Categorical Crossentropy used as loss function\n",
    "\n",
    "model.compile(optimizer = opt,                                                # compiling the model\n",
    "              loss      = loss_fcn,\n",
    "              metrics   = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6dcf63",
   "metadata": {
    "papermill": {
     "duration": 73.51287,
     "end_time": "2025-03-09T09:48:02.729224",
     "exception": false,
     "start_time": "2025-03-09T09:46:49.216354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split of the dataset into train e validation set with a rate of 20%\n",
    "\n",
    "split = 0.2\n",
    "\n",
    "dataset_size = len(list(dataset)) \n",
    "train_size = int((1 - split) * dataset_size)\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=dataset_size, seed=seed) # shuffle the data\n",
    "\n",
    "train_dataset = dataset.take(train_size)  # keep the first 80% of the dataset\n",
    "val_dataset = dataset.skip(train_size)    # Skip the data kept for the training,set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a383164",
   "metadata": {
    "papermill": {
     "duration": 0.020182,
     "end_time": "2025-03-09T09:48:02.761835",
     "exception": false,
     "start_time": "2025-03-09T09:48:02.741653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data augmentation: \n",
    "# random rotation of 5% from original image\n",
    "# random translation of 10% from original image\n",
    "# random zoom of 10% from original image\n",
    "\n",
    "data_augmentation = Sequential([\n",
    "                      keras.layers.RandomRotation(0.05),\n",
    "                      keras.layers.RandomTranslation(0.1,0.1),\n",
    "                      keras.layers.RandomZoom(0.1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008ef399",
   "metadata": {
    "papermill": {
     "duration": 0.206378,
     "end_time": "2025-03-09T09:48:02.975265",
     "exception": false,
     "start_time": "2025-03-09T09:48:02.768887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128 # Use the same batch size for the mapping of the images and for the fitting of the model to avoid bottleneck and memory loss\n",
    "\n",
    "# Apply of data augmentation only to the training images (there is no risk of misalignment).\n",
    "train_dataset = train_dataset.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "\n",
    "# Data's batching and loading optimization\n",
    "train_dataset = train_dataset.shuffle(buffer_size=int(train_dataset.cardinality()), seed=seed).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE) # no shuffle on validation set, we want to compare results\n",
    "# method prefetch: while the GPU trains the current batch, the CPU preloads the next batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8970eb2a",
   "metadata": {
    "papermill": {
     "duration": 0.013077,
     "end_time": "2025-03-09T09:48:02.996034",
     "exception": false,
     "start_time": "2025-03-09T09:48:02.982957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Two callbacks: reduce learning rate and early stopping(so number of epochs are raised)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-7, min_delta=0.005)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, min_delta=0.001)\n",
    "\n",
    "# Patience indicates the number of epochs without improvements before stopping the training.  \n",
    "# The patience of early stopping must be set greater than the patience of reduce_lr.  \n",
    "# Factor represents the reduction factor of the learning rate when the monitored metric (val_loss in your case) stops improving.  \n",
    "# The learning rate controls the step size that the optimization algorithm takes in the direction of minimizing the loss function.  \n",
    "# Val_loss is monitored to prevent overfitting.\n",
    "# min_delta is set to indicate when to apply the callback.\n",
    "\n",
    "# Example for the reduce learning rate: if there isn't a decrement on the validation loss of 0.005(=min_delta) with respect to the previous 2(=patience) epochs\n",
    "# the learning rate will be halved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc83f9a",
   "metadata": {
    "papermill": {
     "duration": 1558.256342,
     "end_time": "2025-03-09T10:14:01.263359",
     "exception": false,
     "start_time": "2025-03-09T09:48:03.007017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model Fit\n",
    "\n",
    "n_epochs   = 50 # very high number of epochs but early stopping prevents useless epochs saving time \n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data = val_dataset,\n",
    "                    epochs          = n_epochs,\n",
    "                    batch_size      = batch_size,\n",
    "                    callbacks       = [reduce_lr, early_stopping],\n",
    "                    verbose         = 1   \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b744368",
   "metadata": {
    "papermill": {
     "duration": 0.553882,
     "end_time": "2025-03-09T10:14:01.980628",
     "exception": false,
     "start_time": "2025-03-09T10:14:01.426746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot of the training graphs: accuracy and loss\n",
    "\n",
    "print(type(history))\n",
    "if type(history) != dict:\n",
    "    history = history.history\n",
    "\n",
    "plt.figure(figsize=(20, 3))\n",
    "for i, metric in enumerate([\"accuracy\", \"loss\"]):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    plt.plot(history[metric])\n",
    "    plt.plot(history[\"val_\" + metric])\n",
    "    plt.title(\"Model {}\".format(metric))\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train\", \"val\"])\n",
    "\n",
    "# print fo the training values\n",
    "print(\"Accuracy: \", history[\"accuracy\"][-1])\n",
    "print(\"Validation Accuracy: \", history[\"val_accuracy\"][-1])\n",
    "\n",
    "print(\"Loss: \", history[\"loss\"][-1])\n",
    "print(\"Validation Loss: \", history[\"val_loss\"][-1])\n",
    "\n",
    "# prints optimal capacity -> at which epoch the model had the higher validation accuracy\n",
    "print(\"Optimal Capacity: \", np.argmax(history[\"val_accuracy\"]) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a9fb6",
   "metadata": {
    "papermill": {
     "duration": 0.306871,
     "end_time": "2025-03-09T10:14:02.449006",
     "exception": false,
     "start_time": "2025-03-09T10:14:02.142135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtaining test labels and correspective images' paths\n",
    "\n",
    "test = pd.read_csv(test_csv_path)   # reads the csv file containing the labels in a data structor that has as columns ClassId and Path\n",
    "test_labels=[]\n",
    "test_path=[]\n",
    "for i in range(len(test)):                          # for every row are saved (in order) in two arrays, the class id and the path of the image\n",
    "    test_labels.append(test[\"ClassId\"][i])\n",
    "    test_path.append(path +'/' + test[\"Path\"][i])\n",
    "\n",
    "test_labels = np.array(test_labels,dtype=int)       # trasforms the labels in integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53e8b8b",
   "metadata": {
    "papermill": {
     "duration": 0.247404,
     "end_time": "2025-03-09T10:14:02.857731",
     "exception": false,
     "start_time": "2025-03-09T10:14:02.610327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create test set \n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_path, test_labels))\n",
    "\n",
    "# Pre-processing of test set\n",
    "test_dataset = test_dataset.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Batch to speed-up the test\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e485a3d6",
   "metadata": {
    "papermill": {
     "duration": 20.316507,
     "end_time": "2025-03-09T10:14:23.334214",
     "exception": false,
     "start_time": "2025-03-09T10:14:03.017707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate prediction of the model\n",
    "predictions = model.predict(test_dataset)\n",
    "\n",
    "# Get the class with the highest probability for each prediction\n",
    "predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3377d5",
   "metadata": {
    "papermill": {
     "duration": 4.373438,
     "end_time": "2025-03-09T10:14:27.878691",
     "exception": false,
     "start_time": "2025-03-09T10:14:23.505253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the images and the labels\n",
    "test_dataset_unbatched = test_dataset.unbatch()\n",
    "x_test, y_test = zip(*test_dataset_unbatched)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0437135c",
   "metadata": {
    "papermill": {
     "duration": 1.007846,
     "end_time": "2025-03-09T10:14:29.055711",
     "exception": false,
     "start_time": "2025-03-09T10:14:28.047865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot of 10 random images with corresponding prediction and true labels\n",
    "\n",
    "plt.figure(figsize=(25, 25))\n",
    "\n",
    "indices = random.sample(range(0, len(x_test)), 10)\n",
    "\n",
    "j=0\n",
    "for i in indices:\n",
    "    j+=1\n",
    "    plt.subplot(5, 5, j)\n",
    "    plt.imshow(x_test[i])\n",
    "    plt.title(\"True: {} {}\\n Predicted: {} {}\".format(y_test[i], label_name[y_test[i]],predictions[i], label_name[predictions[i]]) )\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46be54f9",
   "metadata": {
    "papermill": {
     "duration": 0.172794,
     "end_time": "2025-03-09T10:14:29.417578",
     "exception": false,
     "start_time": "2025-03-09T10:14:29.244784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 82373,
     "sourceId": 191501,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1685.652608,
   "end_time": "2025-03-09T10:14:33.117601",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-09T09:46:27.464993",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
